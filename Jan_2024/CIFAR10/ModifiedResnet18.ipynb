{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in links: https://storage.googleapis.com/jax-releases/jax_releases.html\n",
      "Collecting jax\n",
      "  Using cached https://storage.googleapis.com/jax-releases/jax/jax-0.4.26-py3-none-any.whl (1.9 MB)\n",
      "\u001b[31mERROR: Ignored the following versions that require a different python version: 0.4.14 Requires-Python >=3.9; 0.4.15 Requires-Python >=3.9; 0.4.16 Requires-Python >=3.9; 0.4.17 Requires-Python >=3.9; 0.4.18 Requires-Python >=3.9; 0.4.19 Requires-Python >=3.9; 0.4.20 Requires-Python >=3.9; 0.4.21 Requires-Python >=3.9; 0.4.22 Requires-Python >=3.9; 0.4.23 Requires-Python >=3.9; 0.4.24 Requires-Python >=3.9; 0.4.25 Requires-Python >=3.9; 0.4.26 Requires-Python >=3.9\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement jaxlib==0.1.57+cuda111 (from versions: 0.1.32, 0.1.40, 0.1.41, 0.1.42, 0.1.43, 0.1.44, 0.1.46, 0.1.50, 0.1.51, 0.1.52, 0.1.55, 0.1.56, 0.1.57, 0.1.58, 0.1.59, 0.1.60, 0.1.61, 0.1.62, 0.1.63, 0.1.64, 0.1.65, 0.1.66, 0.1.67, 0.1.68, 0.1.69, 0.1.70, 0.1.71, 0.1.72, 0.1.73, 0.1.74, 0.1.75, 0.1.76, 0.3.0, 0.3.2, 0.3.5, 0.3.7, 0.3.8, 0.3.10, 0.3.14, 0.3.15, 0.3.18, 0.3.20, 0.3.22, 0.3.24, 0.3.25, 0.4.1, 0.4.2, 0.4.3, 0.4.4, 0.4.6, 0.4.7, 0.4.9, 0.4.10, 0.4.11, 0.4.12, 0.4.13)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for jaxlib==0.1.57+cuda111\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade jax jaxlib==0.1.57+cuda111 -f https://storage.googleapis.com/jax-releases/jax_releases.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-13 14:02:38.073104: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-13 14:02:38.570235: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n",
    "from tensorflow.keras.layers import Input, Add, Dense, Layer, Activation, concatenate,Conv2D, Flatten, GlobalAveragePooling2D ,BatchNormalization, Dropout\n",
    "from keras.regularizers import l2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Clear the session\n",
    "# K.clear_session()\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "class PrintShapeCallback(Callback):\n",
    "    def __init__(self, model_layer_name):\n",
    "        super(PrintShapeCallback, self).__init__()\n",
    "        self.model_layer_name = model_layer_name\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Get the output of the layer with the specified name\n",
    "        layer_output = self.model.get_layer(self.model_layer_name).output\n",
    "        print(f\"After epoch {epoch+1}, shape of x (from layer {self.model_layer_name}): {layer_output.shape}\")\n",
    "\n",
    "print_shape_callback = PrintShapeCallback(model_layer_name='dense_1')  # Assuming 'dense_1' is the name of the Dense layer after h2(x)\n",
    "\n",
    "class StopAtThresholdCallback(Callback):\n",
    "    def __init__(self, threshold):\n",
    "        super(StopAtThresholdCallback, self).__init__()\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        val_loss = logs.get('val_loss')\n",
    "        if val_loss is not None and val_loss < self.threshold:\n",
    "            print(f\"\\nStopping training as validation loss {val_loss} is below the threshold {self.threshold}\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "# callback = StopAtThresholdCallback(threshold=1e-03)\n",
    "callback = StopAtThresholdCallback(threshold=9.8023e-06)\n",
    "\n",
    "class H1Layer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(H1Layer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.b = self.add_weight(shape=(input_shape[-1],),\n",
    "                                initializer=RandomNormal(mean=0.0,stddev=0.03),\n",
    "                                trainable=True)\n",
    "        super(H1Layer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        return (self.b * (2 * x))/ (2**(1/2) * np.pi**(1/4))\n",
    "        #return (2 * x) \n",
    "\n",
    "\n",
    "class H2Layer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(H2Layer, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, x, h1):\n",
    "        return ((2*x*(h1))-2)/(2*(np.pi**(1/4))*np.sqrt(math.factorial(2)))\n",
    "    \n",
    "class H3Layer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(H3Layer, self).__init__(**kwargs)\n",
    "        \n",
    "    def call(self, x, h1, h2):\n",
    "        return ((2 * x * (h2))-(4 * h1)) / (2**(3/2)*(np.pi**(1/4))*np.sqrt(math.factorial(3)))\n",
    "\n",
    "class H4Layer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(H4Layer, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, x, h2, h3):\n",
    "        return ((2*x*(h3))-(6*h2)) / (2**2 *(np.pi**(1/4))*np.sqrt(math.factorial(4)))\n",
    "\n",
    "class H5Layer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(H5Layer,self).__init__(**kwargs)\n",
    "\n",
    "    def call(self,x, h3, h4):\n",
    "        return ((2*x*(h4))-(8*h3)) / (2**(5/2) * (np.pi**(1/4))*np.sqrt(math.factorial(5)))\n",
    "    \n",
    "class H6Layer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(H6Layer,self).__init__(**kwargs)\n",
    "        \n",
    "    def call(self,x, h4, h5):\n",
    "        return (2*x*(h5))-(10*h4)\n",
    "\n",
    "class TensorDecompositionLayer(Layer):\n",
    "    def __init__(self, rank, **kwargs):\n",
    "        self.rank = rank\n",
    "        super(TensorDecompositionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.factors_a = self.add_weight(shape=(input_shape[-1], self.rank),\n",
    "                                         initializer=RandomNormal(mean=0.0,stddev=0.05),\n",
    "                                         trainable=True)\n",
    "        self.factors_b = self.add_weight(shape=(self.rank, input_shape[-1]),\n",
    "                                        initializer=RandomNormal(mean=0.0,stddev=0.05),\n",
    "                                        trainable=True)\n",
    "        super(TensorDecompositionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        return tf.matmul(tf.matmul(x, self.factors_a), self.factors_b)\n",
    "\n",
    " \n",
    "class Relu_With_Weight(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Relu_With_Weight, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.b = self.add_weight(shape=(input_shape[-1],),\n",
    "                                initializer=RandomNormal(),\n",
    "                                trainable=True)\n",
    "        super(Relu_With_Weight, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        return K.tanh(x * self.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR 10 RESNET 18 OUR APPROACH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-13 14:02:40.919605: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-13 14:02:40.937716: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-13 14:02:40.937908: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-13 14:02:40.939889: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysf"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method ResNetHermiteBlock.call of <__main__.ResNetHermiteBlock object at 0x7fda445398e0>> and will run it as-is.\n",
      "Cause: mangled names are not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method ResNetHermiteBlock.call of <__main__.ResNetHermiteBlock object at 0x7fda445398e0>> and will run it as-is.\n",
      "Cause: mangled names are not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s-bus-pci#L344-L355\n",
      "2024-05-13 14:02:40.940032: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-13 14:02:40.940158: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-13 14:02:41.346842: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-13 14:02:41.347007: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-13 14:02:41.347137: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-13 14:02:41.347236: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 45824 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:4d:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"res_net18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             multiple                  9472      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           multiple                  200768    \n",
      "                                                                 \n",
      " batch_normalization (Batch  multiple                  0 (unused)\n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " h1_layer (H1Layer)          multiple                  64        \n",
      "                                                                 \n",
      " h2_layer (H2Layer)          multiple                  0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  multiple                  0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " res_net_hermite_block (Res  multiple                  73920     \n",
      " NetHermiteBlock)                                                \n",
      "                                                                 \n",
      " res_net_hermite_block_1 (R  multiple                  0 (unused)\n",
      " esNetHermiteBlock)                                              \n",
      "                                                                 \n",
      " res_net_hermite_block_2 (R  multiple                  230400    \n",
      " esNetHermiteBlock)                                              \n",
      "                                                                 \n",
      " res_net_hermite_block_3 (R  multiple                  0 (unused)\n",
      " esNetHermiteBlock)                                              \n",
      "                                                                 \n",
      " res_net_hermite_block_4 (R  multiple                  919552    \n",
      " esNetHermiteBlock)                                              \n",
      "                                                                 \n",
      " res_net_hermite_block_5 (R  multiple                  0 (unused)\n",
      " esNetHermiteBlock)                                              \n",
      "                                                                 \n",
      " res_net_hermite_block_6 (R  multiple                  3674112   \n",
      " esNetHermiteBlock)                                              \n",
      "                                                                 \n",
      " res_net_hermite_block_7 (R  multiple                  0 (unused)\n",
      " esNetHermiteBlock)                                              \n",
      "                                                                 \n",
      " global_average_pooling2d (  multiple                  0         \n",
      " GlobalAveragePooling2D)                                         \n",
      "                                                                 \n",
      " flatten (Flatten)           multiple                  0         \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5113418 (19.51 MB)\n",
      "Trainable params: 5111626 (19.50 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-13 14:02:44.461308: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2024-05-13 14:02:45.292352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-05-13 14:02:45.293467: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1f068a10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-05-13 14:02:45.293481: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2024-05-13 14:02:45.296981: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-05-13 14:02:45.397082: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156/156 [==============================] - 18s 69ms/step - loss: 1.8973 - accuracy: 0.3167 - val_loss: 44.9739 - val_accuracy: 0.0922\n",
      "Epoch 2/50\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.5281 - accuracy: 0.4567 - val_loss: 36.8798 - val_accuracy: 0.0922\n",
      "Epoch 3/50\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 1.3942 - accuracy: 0.5038 - val_loss: 60.2350 - val_accuracy: 0.0964\n",
      "Epoch 4/50\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 1.3050 - accuracy: 0.5394 - val_loss: 11.4916 - val_accuracy: 0.1916\n",
      "Epoch 5/50\n",
      "156/156 [==============================] - 10s 62ms/step - loss: 1.2368 - accuracy: 0.5633 - val_loss: 37.4201 - val_accuracy: 0.1392\n",
      "Epoch 6/50\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.1893 - accuracy: 0.5807 - val_loss: 85.3373 - val_accuracy: 0.1118\n",
      "Epoch 7/50\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 1.1228 - accuracy: 0.6052 - val_loss: 33.2994 - val_accuracy: 0.1208\n",
      "Epoch 8/50\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 1.0756 - accuracy: 0.6211 - val_loss: 10.5518 - val_accuracy: 0.1592\n",
      "Epoch 9/50\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 1.0166 - accuracy: 0.6392 - val_loss: 36.3025 - val_accuracy: 0.1954\n",
      "Epoch 10/50\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.9660 - accuracy: 0.6591 - val_loss: 11.1894 - val_accuracy: 0.1016\n",
      "Epoch 11/50\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 0.9199 - accuracy: 0.6778 - val_loss: 10.0276 - val_accuracy: 0.1598\n",
      "Epoch 12/50\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 0.8839 - accuracy: 0.6926 - val_loss: 16.9638 - val_accuracy: 0.1342\n",
      "Epoch 13/50\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 0.8396 - accuracy: 0.7044 - val_loss: 360.7787 - val_accuracy: 0.1080\n",
      "Epoch 14/50\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 0.8166 - accuracy: 0.7143 - val_loss: 15.9907 - val_accuracy: 0.1872\n",
      "Epoch 15/50\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 0.7801 - accuracy: 0.7263 - val_loss: 8.7688 - val_accuracy: 0.3228\n",
      "Epoch 16/50\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.7467 - accuracy: 0.7395 - val_loss: 12.0380 - val_accuracy: 0.1700\n",
      "Epoch 17/50\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 0.7323 - accuracy: 0.7448 - val_loss: 132.6928 - val_accuracy: 0.1298\n",
      "Epoch 18/50\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 0.7041 - accuracy: 0.7524 - val_loss: 16.7666 - val_accuracy: 0.2218\n",
      "Epoch 19/50\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 0.6713 - accuracy: 0.7632 - val_loss: 21.4452 - val_accuracy: 0.2426\n",
      "Epoch 20/50\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 0.6363 - accuracy: 0.7776 - val_loss: 5.7974 - val_accuracy: 0.3180\n",
      "Epoch 21/50\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 0.6216 - accuracy: 0.7822 - val_loss: 4.4042 - val_accuracy: 0.3290\n",
      "Epoch 22/50\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 0.5905 - accuracy: 0.7930 - val_loss: 11.6272 - val_accuracy: 0.3674\n",
      "Epoch 23/50\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 0.5789 - accuracy: 0.7973 - val_loss: 4.5420 - val_accuracy: 0.5116\n",
      "Epoch 24/50\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.5587 - accuracy: 0.8045 - val_loss: 9.5351 - val_accuracy: 0.1176\n",
      "Epoch 25/50\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.5674 - accuracy: 0.8047 - val_loss: 8.0161 - val_accuracy: 0.3472\n",
      "Epoch 26/50\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 0.5450 - accuracy: 0.8122 - val_loss: 15.2394 - val_accuracy: 0.3084\n",
      "Epoch 27/50\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.5238 - accuracy: 0.8160 - val_loss: 20.1853 - val_accuracy: 0.3020\n",
      "Epoch 28/50\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 0.4897 - accuracy: 0.8290 - val_loss: 8.1462 - val_accuracy: 0.3864\n",
      "Epoch 29/50\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 0.4611 - accuracy: 0.8401 - val_loss: 8.2485 - val_accuracy: 0.2388\n",
      "Epoch 30/50\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.4403 - accuracy: 0.8459 - val_loss: 2.3249 - val_accuracy: 0.5644\n",
      "Epoch 31/50\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.4739 - accuracy: 0.8372 - val_loss: 4.3306 - val_accuracy: 0.4032\n",
      "Epoch 32/50\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 0.4293 - accuracy: 0.8496 - val_loss: 5.4301 - val_accuracy: 0.3718\n",
      "Epoch 33/50\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 0.4658 - accuracy: 0.8429 - val_loss: 97.1618 - val_accuracy: 0.0784\n",
      "Epoch 34/50\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.4431 - accuracy: 0.8458 - val_loss: 7.7079 - val_accuracy: 0.4714\n",
      "Epoch 35/50\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.3833 - accuracy: 0.8664 - val_loss: 3.2258 - val_accuracy: 0.5564\n",
      "Epoch 36/50\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.3681 - accuracy: 0.8727 - val_loss: 3.3458 - val_accuracy: 0.5218\n",
      "Epoch 37/50\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 0.3587 - accuracy: 0.8768 - val_loss: 3.4898 - val_accuracy: 0.5568\n",
      "Epoch 38/50\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 0.3569 - accuracy: 0.8784 - val_loss: 9.9484 - val_accuracy: 0.4784\n",
      "Epoch 39/50\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 0.3657 - accuracy: 0.8755 - val_loss: 6.0775 - val_accuracy: 0.4034\n",
      "Epoch 40/50\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.3508 - accuracy: 0.8794 - val_loss: 4.5492 - val_accuracy: 0.5184\n",
      "Epoch 41/50\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 0.3273 - accuracy: 0.8871 - val_loss: 3.0339 - val_accuracy: 0.6174\n",
      "Epoch 42/50\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 0.3184 - accuracy: 0.8900 - val_loss: 4.9269 - val_accuracy: 0.4738\n",
      "Epoch 43/50\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.3211 - accuracy: 0.8900 - val_loss: 2.3425 - val_accuracy: 0.6398\n",
      "Epoch 44/50\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.3021 - accuracy: 0.8955 - val_loss: 2.8602 - val_accuracy: 0.6504\n",
      "Epoch 45/50\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.2933 - accuracy: 0.9004 - val_loss: 3.1613 - val_accuracy: 0.6008\n",
      "Epoch 46/50\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 0.2882 - accuracy: 0.9025 - val_loss: 2.1498 - val_accuracy: 0.6814\n",
      "Epoch 47/50\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 0.2721 - accuracy: 0.9079 - val_loss: 2.9337 - val_accuracy: 0.6092\n",
      "Epoch 48/50\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.2646 - accuracy: 0.9090 - val_loss: 3.4669 - val_accuracy: 0.6052\n",
      "Epoch 49/50\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 0.2515 - accuracy: 0.9140 - val_loss: 4.5585 - val_accuracy: 0.5132\n",
      "Epoch 50/50\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.2498 - accuracy: 0.9132 - val_loss: 3.1815 - val_accuracy: 0.5482\n"
     ]
    }
   ],
   "source": [
    "#Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras import datasets,models,layers\n",
    "# Adding TF Cifar10 Data ..\n",
    "from keras.datasets import cifar10\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "# Normalize the data.\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "# Split the data into training, validation, and testing sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "y_train = encoder.fit_transform(y_train.reshape(-1, 1)).toarray()\n",
    "y_val = encoder.transform(y_val.reshape(-1, 1)).toarray()\n",
    "y_test = encoder.transform(y_test.reshape(-1, 1)).toarray()\n",
    "\n",
    "# Perform data augmentation\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "aug = ImageDataGenerator(horizontal_flip=True, width_shift_range=0.05, height_shift_range=0.05)\n",
    "aug.fit(X_train)\n",
    "\"\"\"\n",
    "ResNet-18\n",
    "Reference:\n",
    "[1] K. He et al. Deep Residual Learning for Image Recognition. CVPR, 2016\n",
    "[2] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectifiers:\n",
    "Surpassing human-level performance on imagenet classification. In\n",
    "ICCV, 2015.\n",
    "\"\"\"\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense, Conv2D,  MaxPool2D, Flatten, GlobalAveragePooling2D,  BatchNormalization, Layer, Add\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class ResNetHermiteBlock(Model):\n",
    "    \"\"\"\n",
    "    A HERMITE resnet block.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels: int, down_sample=False):\n",
    "        \"\"\"\n",
    "        channels: same as number of convolution kernels\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.__channels = channels\n",
    "        self.__down_sample = down_sample\n",
    "        self.__strides = [2, 1] if down_sample else [1, 1]\n",
    "\n",
    "        KERNEL_SIZE = (3, 3)\n",
    "        # use He initialization, instead of Xavier (a.k.a 'glorot_uniform' in Keras), as suggested in [2]\n",
    "        INIT_SCHEME = \"he_normal\"\n",
    "\n",
    "        self.conv_1 = Conv2D(self.__channels, strides=self.__strides[0],\n",
    "                             kernel_size=KERNEL_SIZE, padding=\"same\", kernel_initializer=INIT_SCHEME)\n",
    "        self.bn_1 = BatchNormalization()\n",
    "        self.conv_2 = Conv2D(self.__channels, strides=self.__strides[1],\n",
    "                             kernel_size=KERNEL_SIZE, padding=\"same\", kernel_initializer=INIT_SCHEME)\n",
    "        #self.conv_3= Conv2D(self.__channels, strides=self.__strides[1],kernel_size=KERNEL_SIZE, padding=\"same\", kernel_initializer=INIT_SCHEME)\n",
    "        self.bn_2 = BatchNormalization()\n",
    "        self.merge = Add()\n",
    "\n",
    "        self.tensorDecomp = TensorDecompositionLayer(3)\n",
    "        # Initialize Hermite Polynomial layers\n",
    "        self.h1 = H1Layer()\n",
    "        self.h2 = H2Layer()\n",
    "        self.h3 = H3Layer()\n",
    "\n",
    "        if self.__down_sample:\n",
    "            # perform down sampling using stride of 2, according to [1].\n",
    "            self.res_conv = Conv2D(\n",
    "                self.__channels, strides=2, kernel_size=(1, 1), kernel_initializer=INIT_SCHEME, padding=\"same\")\n",
    "            self.res_bn = BatchNormalization()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        res = inputs\n",
    "        \n",
    "        x = self.conv_1(inputs)\n",
    "        #x = self.bn_1(x)\n",
    "       \n",
    "  \n",
    "        x = x_h1 = self.h1(x)  # Store H1 output if needed for H2\n",
    "        #x = tf.nn.relu(x)\n",
    "        x = self.conv_2(x)\n",
    "        #x = self.bn_2(x)\n",
    "        x = x_h2 = self.h2(x, x_h1)  # Pass x_h1 as the additional argument to H2Layer\n",
    "        #x = x_h3 = self.h3(x, x_h1,x_h2)\n",
    "        #x = self.tensorDecomp(x)\n",
    "\n",
    "        if self.__down_sample:\n",
    "            res = self.res_conv(res)\n",
    "            res = self.res_bn(res)\n",
    "\n",
    "        # if not perform down sample, then add a shortcut directly\n",
    "        out = self.merge([x, res])\n",
    "        # out = tf.nn.relu(x)\n",
    "        # x_h1 = self.h1(x)  # Store H1 output if needed for H2\n",
    "        # out = x_h2 = self.h2(x, x_h1)  # Pass x_h1 as the additional argument to H2Layer\n",
    "        # out = self.tensorDecomp(x)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet18(Model):\n",
    "\n",
    "    def __init__(self, num_classes, **kwargs):\n",
    "        \"\"\"\n",
    "            num_classes: number of classes in specific classification task.\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.conv_1 = Conv2D(64, (7, 7), strides=2,\n",
    "                             padding=\"same\", kernel_initializer=\"he_normal\")\n",
    "        self.conv_2 = Conv2D(64, (7, 7),\n",
    "                             padding=\"same\", kernel_initializer=\"he_normal\")\n",
    "        \n",
    "        self.init_bn = BatchNormalization()\n",
    "        self.h1 = H1Layer()\n",
    "        self.h2 = H2Layer()\n",
    "        self.pool_2 = MaxPool2D(pool_size=(2, 2), strides=2, padding=\"same\")\n",
    "        self.res_1_1 = ResNetHermiteBlock(64)\n",
    "        self.res_1_2 = ResNetHermiteBlock(64)\n",
    "        self.res_2_1 = ResNetHermiteBlock(128, down_sample=True)\n",
    "        self.res_2_2 = ResNetHermiteBlock(128)\n",
    "        self.res_3_1 = ResNetHermiteBlock(256, down_sample=True)\n",
    "        self.res_3_2 = ResNetHermiteBlock(256)\n",
    "        self.res_4_1 = ResNetHermiteBlock(512, down_sample=True)\n",
    "        self.res_4_2 = ResNetHermiteBlock(512)\n",
    "        #self.res_5_1 = ResNetHermiteBlock(1024, down_sample=True)\n",
    "        self.avg_pool = GlobalAveragePooling2D()\n",
    "        self.flat = Flatten()\n",
    "        self.fc = Dense(num_classes, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        out = self.conv_1(inputs)\n",
    "        #out = self.init_bn(out)\n",
    "        out = x_h1 = self.h1(out)\n",
    "        out = self.conv_2(out)\n",
    "        out = x_h2 = self.h2(out, x_h1)\n",
    "        # out = tf.nn.relu(out)\n",
    "        out = self.pool_2(out)\n",
    "\n",
    "        #for res_block in [self.res_1_1, self.res_1_2, self.res_2_1, self.res_2_2, self.res_3_1, self.res_3_2, self.res_4_1, self.res_4_2]:\n",
    "        for res_block in [self.res_1_1, self.res_2_1, self.res_3_1, self.res_4_1]:\n",
    "            out = res_block(out)\n",
    "        out = self.avg_pool(out)\n",
    "        out = self.flat(out)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "model = ResNet18(10)\n",
    "model.build(input_shape = (None,32,32,3))\n",
    "#use categorical_crossentropy since the label is one-hot encoded\n",
    "from keras.optimizers import SGD\n",
    "# opt = SGD(learning_rate=0.1,momentum=0.9,decay = 1e-04) #parameters suggested by He [1]\n",
    "model.compile(optimizer = \"adam\",loss='categorical_crossentropy', metrics=[\"accuracy\"]) #LR = 0.001\n",
    "model.summary()\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "es = EarlyStopping(patience= 8, restore_best_weights=True, monitor=\"val_accuracy\")\n",
    "#I did not use cross validation, so the validate performance is not accurate.\n",
    "STEPS = len(X_train) / 256\n",
    "#history = model.fit(aug.flow(X_train,Y_train,batch_size = 256), steps_per_epoch=STEPS, batch_size = 256, epochs=60, validation_data=(X_train, Y_train),callbacks=[es])\n",
    "history = model.fit(aug.flow(X_train,y_train,batch_size = 256), steps_per_epoch=STEPS, batch_size = 256, epochs=50, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# list all data in history\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mhistory\u001b[49m\u001b[38;5;241m.\u001b[39mhistory\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m     22\u001b[0m plotmodelhistory(history)\n\u001b[1;32m     24\u001b[0m X_test \u001b[38;5;241m=\u001b[39m X_test\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def plotmodelhistory(history): \n",
    "    fig, axs = plt.subplots(1,2,figsize=(15,5)) \n",
    "    # summarize history for accuracy\n",
    "    axs[0].plot(history.history['accuracy']) \n",
    "    axs[0].plot(history.history['val_accuracy']) \n",
    "    axs[0].set_title('Model Accuracy')\n",
    "    axs[0].set_ylabel('Accuracy') \n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    \n",
    "    axs[0].legend(['train', 'validate'], loc='upper left')\n",
    "    # summarize history for loss\n",
    "    axs[1].plot(history.history['loss']) \n",
    "    axs[1].plot(history.history['val_loss']) \n",
    "    axs[1].set_title('Model Loss')\n",
    "    axs[1].set_ylabel('Loss') \n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].legend(['train', 'validate'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "plotmodelhistory(history)\n",
    "\n",
    "X_test = X_test\n",
    "y_true = y_test\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "val_loss, val_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Validation accuracy: {val_acc:.4f}\")\n",
    "# Assuming y_true and y_pred are already one-hot encoded, convert them back to class labels\n",
    "y_true_labels = np.argmax(y_true, axis=1)\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Plotting a confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm = confusion_matrix(y_true_labels, y_pred_labels)\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual Labels')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 32, 32, 3)\n",
      "(10000, 32, 32, 3)\n",
      "(10000, 32, 32, 3)\n",
      "(40000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(X_val.shape)\n",
    "print(Y_train.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
