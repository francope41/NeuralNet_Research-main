{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import gc\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n",
    "from tensorflow.keras.layers import Input, Add, Dense, Layer, Activation, concatenate,Conv2D, Flatten, MaxPooling2D,BatchNormalization, Dropout\n",
    "from keras.regularizers import l2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "# Clear the session\n",
    "# K.clear_session()\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "class PrintShapeCallback(Callback):\n",
    "    def __init__(self, model_layer_name):\n",
    "        super(PrintShapeCallback, self).__init__()\n",
    "        self.model_layer_name = model_layer_name\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Get the output of the layer with the specified name\n",
    "        layer_output = self.model.get_layer(self.model_layer_name).output\n",
    "        print(f\"After epoch {epoch+1}, shape of x (from layer {self.model_layer_name}): {layer_output.shape}\")\n",
    "\n",
    "print_shape_callback = PrintShapeCallback(model_layer_name='dense_1')  # Assuming 'dense_1' is the name of the Dense layer after h2(x)\n",
    "\n",
    "class StopAtThresholdCallback(Callback):\n",
    "    def __init__(self, threshold):\n",
    "        super(StopAtThresholdCallback, self).__init__()\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        val_loss = logs.get('val_loss')\n",
    "        if val_loss is not None and val_loss < self.threshold:\n",
    "            print(f\"\\nStopping training as validation loss {val_loss} is below the threshold {self.threshold}\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "# callback = StopAtThresholdCallback(threshold=1e-03)\n",
    "callback = StopAtThresholdCallback(threshold=1.2962e-07)\n",
    "\n",
    "class H1Layer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(H1Layer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.b = self.add_weight(shape=(input_shape[-1],),\n",
    "                                initializer=RandomNormal(mean=0.0,stddev=0.03),\n",
    "                                trainable=True)\n",
    "        super(H1Layer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        return (self.b * (2 * x))/ (2**(1/2) * np.pi**(1/4))\n",
    "        #return (2 * x) \n",
    "\n",
    "\n",
    "class H2Layer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(H2Layer, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, x, h1):\n",
    "        return (((2*x*(h1)))-2)/(2*(np.pi**(1/4))*np.sqrt(math.factorial(2)))\n",
    "    \n",
    "class H3Layer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(H3Layer, self).__init__(**kwargs)\n",
    "        \n",
    "    def call(self, x, h1, h2):\n",
    "        return (((2 * x * (h2)))-(4 * h1)) / (2**(3/2)*(np.pi**(1/4))*np.sqrt(math.factorial(3)))\n",
    "\n",
    "class H4Layer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(H4Layer, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, x, h2, h3):\n",
    "        return (((2*x*(h3)))-(6*h2)) / (2**2 *(np.pi**(1/4))*np.sqrt(math.factorial(4)))\n",
    "\n",
    "class H5Layer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(H5Layer,self).__init__(**kwargs)\n",
    "\n",
    "    def call(self,x, h3, h4):\n",
    "        return (((2 * x * h4)) - (8 * h3)) / (2**(5/2) * (np.pi**(1/4)) * np.sqrt(math.factorial(5)))\n",
    "\n",
    "#FROM THIS ON NO MODIFICATION ON WEIGHTS\n",
    "class H6Layer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(H6Layer,self).__init__(**kwargs)\n",
    "        \n",
    "    def call(self,x, h4, h5):\n",
    "        return (2*x*(h5))-(10*h4)\n",
    "\n",
    "class TensorDecompositionLayer(Layer):\n",
    "    def __init__(self, rank, **kwargs):\n",
    "        self.rank = rank\n",
    "        super(TensorDecompositionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.factors_a = self.add_weight(shape=(input_shape[-1], self.rank),\n",
    "                                         initializer=RandomNormal(mean=0.0,stddev=0.05),\n",
    "                                         trainable=True)\n",
    "        self.factors_b = self.add_weight(shape=(self.rank, input_shape[-1]),\n",
    "                                        initializer=RandomNormal(mean=0.0,stddev=0.05),\n",
    "                                        trainable=True)\n",
    "        super(TensorDecompositionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        return tf.matmul(tf.matmul(x, self.factors_a), self.factors_b)\n",
    "\n",
    " \n",
    "class Relu_With_Weight(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Relu_With_Weight, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.b = self.add_weight(shape=(input_shape[-1],),\n",
    "                                initializer=RandomNormal(),\n",
    "                                trainable=True)\n",
    "        super(Relu_With_Weight, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        return K.tanh(x * self.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 32, 32, 32)       128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 32, 32, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 32, 32, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 16, 16, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16, 16, 32)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 16, 16, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 16, 16, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 16, 16, 64)        36928     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 16, 16, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 8, 8, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 8, 8, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 8, 8, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 4, 4, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 128)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 306,602\n",
      "Trainable params: 305,706\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 13:45:00.001593: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-06-05 13:45:00.018146: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-06-05 13:45:00.018300: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-06-05 13:45:00.018670: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-05 13:45:00.019281: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-06-05 13:45:00.019414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-06-05 13:45:00.019523: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-06-05 13:45:00.067091: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-06-05 13:45:00.067244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-06-05 13:45:00.067333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-06-05 13:45:00.067406: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5650 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Ti, pci bus id: 0000:02:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 13:45:01.597718: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8600\n",
      "2024-06-05 13:45:01.782272: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 7s 8ms/step - loss: 1.4977 - accuracy: 0.4607 - val_loss: 1.3477 - val_accuracy: 0.5237\n",
      "Epoch 2/50\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 1.0795 - accuracy: 0.6195 - val_loss: 0.9830 - val_accuracy: 0.6463\n",
      "Epoch 3/50\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.9147 - accuracy: 0.6814 - val_loss: 0.8246 - val_accuracy: 0.7091\n",
      "Epoch 4/50\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.8085 - accuracy: 0.7231 - val_loss: 0.7722 - val_accuracy: 0.7256\n",
      "Epoch 5/50\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.7283 - accuracy: 0.7506 - val_loss: 0.7018 - val_accuracy: 0.7591\n",
      "Epoch 6/50\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.6708 - accuracy: 0.7719 - val_loss: 0.7145 - val_accuracy: 0.7571\n",
      "Epoch 7/50\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.6237 - accuracy: 0.7881 - val_loss: 0.6156 - val_accuracy: 0.7893\n",
      "Epoch 8/50\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.5876 - accuracy: 0.8008 - val_loss: 0.5931 - val_accuracy: 0.7967\n",
      "Epoch 9/50\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.5567 - accuracy: 0.8124 - val_loss: 0.6172 - val_accuracy: 0.7947\n",
      "Epoch 10/50\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.5360 - accuracy: 0.8179 - val_loss: 0.5435 - val_accuracy: 0.8114\n",
      "Epoch 11/50\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.5096 - accuracy: 0.8267 - val_loss: 0.5055 - val_accuracy: 0.8296\n",
      "Epoch 12/50\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.4979 - accuracy: 0.8300 - val_loss: 0.5294 - val_accuracy: 0.8194\n",
      "Epoch 13/50\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.4739 - accuracy: 0.8400 - val_loss: 0.5719 - val_accuracy: 0.8164\n",
      "Epoch 14/50\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.4591 - accuracy: 0.8456 - val_loss: 0.5350 - val_accuracy: 0.8193\n",
      "Epoch 15/50\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.4452 - accuracy: 0.8490 - val_loss: 0.7438 - val_accuracy: 0.7619\n",
      "Epoch 16/50\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.4264 - accuracy: 0.8539 - val_loss: 0.5365 - val_accuracy: 0.8259\n",
      "Epoch 17/50\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.4181 - accuracy: 0.8586 - val_loss: 0.6327 - val_accuracy: 0.7960\n",
      "Epoch 18/50\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.4085 - accuracy: 0.8595 - val_loss: 0.5090 - val_accuracy: 0.8347\n",
      "Epoch 19/50\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.3970 - accuracy: 0.8650 - val_loss: 0.7917 - val_accuracy: 0.7591\n",
      "Epoch 20/50\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.3875 - accuracy: 0.8672 - val_loss: 0.4438 - val_accuracy: 0.8502\n",
      "Epoch 21/50\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.3789 - accuracy: 0.8719 - val_loss: 0.4773 - val_accuracy: 0.8510\n",
      "Epoch 22/50\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.3673 - accuracy: 0.8742 - val_loss: 0.4847 - val_accuracy: 0.8458\n",
      "Epoch 23/50\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.3599 - accuracy: 0.8771 - val_loss: 0.4810 - val_accuracy: 0.8449\n",
      "Epoch 24/50\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.3551 - accuracy: 0.8798 - val_loss: 0.4898 - val_accuracy: 0.8464\n",
      "Epoch 25/50\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.3438 - accuracy: 0.8806 - val_loss: 0.8136 - val_accuracy: 0.7629\n",
      "Epoch 26/50\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.3429 - accuracy: 0.8847 - val_loss: 0.4642 - val_accuracy: 0.8503\n",
      "Epoch 27/50\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.3363 - accuracy: 0.8849 - val_loss: 0.5158 - val_accuracy: 0.8413\n",
      "Epoch 28/50\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.3300 - accuracy: 0.8881 - val_loss: 0.4538 - val_accuracy: 0.8563\n",
      "Epoch 29/50\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.3229 - accuracy: 0.8895 - val_loss: 0.4819 - val_accuracy: 0.8465\n",
      "Epoch 30/50\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.3167 - accuracy: 0.8919 - val_loss: 0.5295 - val_accuracy: 0.8337\n",
      "Epoch 31/50\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.3156 - accuracy: 0.8913 - val_loss: 0.5156 - val_accuracy: 0.8490\n",
      "Epoch 32/50\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.3098 - accuracy: 0.8944 - val_loss: 0.4173 - val_accuracy: 0.8631\n",
      "Epoch 33/50\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.3022 - accuracy: 0.8966 - val_loss: 0.4281 - val_accuracy: 0.8650\n",
      "Epoch 34/50\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.3008 - accuracy: 0.8969 - val_loss: 0.4388 - val_accuracy: 0.8624\n",
      "Epoch 35/50\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.2988 - accuracy: 0.8985 - val_loss: 0.4371 - val_accuracy: 0.8609\n",
      "Epoch 36/50\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.2890 - accuracy: 0.9004 - val_loss: 0.4950 - val_accuracy: 0.8544\n",
      "Epoch 37/50\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.2861 - accuracy: 0.9019 - val_loss: 0.4775 - val_accuracy: 0.8561\n",
      "Epoch 38/50\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.2842 - accuracy: 0.9021 - val_loss: 0.4258 - val_accuracy: 0.8644\n",
      "Epoch 39/50\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.2761 - accuracy: 0.9048 - val_loss: 0.5066 - val_accuracy: 0.8529\n",
      "Epoch 40/50\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.2687 - accuracy: 0.9065 - val_loss: 0.4637 - val_accuracy: 0.8605\n",
      "Epoch 41/50\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.2763 - accuracy: 0.9042 - val_loss: 0.4209 - val_accuracy: 0.8718\n",
      "Epoch 42/50\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.2643 - accuracy: 0.9083 - val_loss: 0.4233 - val_accuracy: 0.8715\n",
      "Epoch 43/50\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.2642 - accuracy: 0.9087 - val_loss: 0.4417 - val_accuracy: 0.8663\n",
      "Epoch 44/50\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.2630 - accuracy: 0.9082 - val_loss: 0.4682 - val_accuracy: 0.8599\n",
      "Epoch 45/50\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.2519 - accuracy: 0.9119 - val_loss: 0.5122 - val_accuracy: 0.8491\n",
      "Epoch 46/50\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.2586 - accuracy: 0.9117 - val_loss: 0.4690 - val_accuracy: 0.8616\n",
      "Epoch 47/50\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.2533 - accuracy: 0.9134 - val_loss: 0.4539 - val_accuracy: 0.8678\n",
      "Epoch 48/50\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.2535 - accuracy: 0.9120 - val_loss: 0.4721 - val_accuracy: 0.8595\n",
      "Epoch 49/50\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.2457 - accuracy: 0.9158 - val_loss: 0.4094 - val_accuracy: 0.8731\n",
      "Epoch 50/50\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.2452 - accuracy: 0.9154 - val_loss: 0.4244 - val_accuracy: 0.8690\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.4244 - accuracy: 0.8690\n",
      "Test accuracy: 0.8690000176429749\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, GlobalAveragePooling2D\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load and preprocess the CIFAR-10 dataset\n",
    "# Load and preprocess the CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Normalize the images to the range [0, 1]\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Convert class vectors to binary class matrices (one-hot encoding)\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Build the model\n",
    "def build_cifar10_model(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "input_shape = (32, 32, 3)  # CIFAR-10 image shape\n",
    "num_classes = 10\n",
    "model = build_cifar10_model(input_shape, num_classes)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(x_train, y_train, epochs=50, batch_size=64, validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f'Test accuracy: {test_acc}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
