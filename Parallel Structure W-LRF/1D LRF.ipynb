{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Add, Dense, Layer, BatchNormalization,Activation\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import keras.backend as K\n",
    "\n",
    "\n",
    "class PrintShapeCallback(Callback):\n",
    "    def __init__(self, model_layer_name):\n",
    "        super(PrintShapeCallback, self).__init__()\n",
    "        self.model_layer_name = model_layer_name\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Get the output of the layer with the specified name\n",
    "        layer_output = self.model.get_layer(self.model_layer_name).output\n",
    "        print(f\"After epoch {epoch+1}, shape of x (from layer {self.model_layer_name}): {layer_output.shape}\")\n",
    "\n",
    "print_shape_callback = PrintShapeCallback(model_layer_name='dense_1')  # Assuming 'dense_1' is the name of the Dense layer after h2(x)\n",
    "\n",
    "class StopAtThresholdCallback(Callback):\n",
    "    def __init__(self, threshold):\n",
    "        super(StopAtThresholdCallback, self).__init__()\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        val_loss = logs.get('val_loss')\n",
    "        if val_loss is not None and val_loss < self.threshold:\n",
    "            print(f\"\\nStopping training as validation loss {val_loss} is below the threshold {self.threshold}\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "callback = StopAtThresholdCallback(threshold=1e-07)\n",
    "\n",
    "class H1Layer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(H1Layer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.b = self.add_weight(shape=(input_shape[-1],),\n",
    "                                initializer=RandomNormal(mean=0.0,stddev=0.03),\n",
    "                                trainable=True)\n",
    "        super(H1Layer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.b * (2 * x)\n",
    "        #return (2 * x) \n",
    "\n",
    "class H2Layer(Layer):\n",
    "    def __init__(self, h1, **kwargs):\n",
    "        super(H2Layer, self).__init__(**kwargs)\n",
    "        self.h1 = h1\n",
    "\n",
    "    def call(self, x):\n",
    "        return (2*x*(self.h1(x)))-2\n",
    "    \n",
    "class H3Layer(Layer):\n",
    "    def __init__(self, h1, h2, **kwargs):\n",
    "        super(H3Layer, self).__init__(**kwargs)\n",
    "        self.h1 = h1\n",
    "        self.h2 = h2\n",
    "        \n",
    "    def call(self, x):\n",
    "        return (2*x*(self.h2(x)))-(4*self.h1(x))\n",
    "\n",
    "class H4Layer(Layer):\n",
    "    def __init__(self, h2, h3, **kwargs):\n",
    "        super(H4Layer, self).__init__(**kwargs)\n",
    "        self.h2 = h2\n",
    "        self.h3 = h3\n",
    "\n",
    "    def call(self, x):\n",
    "        return (2*x*(self.h3(x)))-(6*self.h2(x))   \n",
    "\n",
    "class H5Layer(Layer):\n",
    "    def __init__(self, h3, h4, **kwargs):\n",
    "        super(H5Layer,self).__init__(**kwargs)\n",
    "        self.h3 = h3\n",
    "        self.h4 = h4\n",
    "\n",
    "    def call(self,x):\n",
    "        return (2*x*(self.h4(x)))-(8*self.h3(x))\n",
    "    \n",
    "class H6Layer(Layer):\n",
    "    def __init__(self, h4, h5, **kwargs):\n",
    "        super(H6Layer,self).__init__(**kwargs)\n",
    "        self.h4 = h4\n",
    "        self.h5 = h5\n",
    "\n",
    "    def call(self,x):\n",
    "        return (2*x*(self.h5(x)))-(10*self.h4(x))\n",
    "\n",
    "class TensorDecompositionLayer(Layer):\n",
    "    def __init__(self, rank, **kwargs):\n",
    "        self.rank = rank\n",
    "        super(TensorDecompositionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.factors_a = self.add_weight(shape=(input_shape[-1], self.rank),\n",
    "                                         initializer='random_normal',\n",
    "                                         trainable=True)\n",
    "        self.factors_b = self.add_weight(shape=(self.rank, input_shape[-1]),\n",
    "                                         initializer='random_normal',\n",
    "                                         trainable=True)\n",
    "        super(TensorDecompositionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        return tf.matmul(tf.matmul(x, self.factors_a), self.factors_b)\n",
    "    \n",
    "class Relu_With_Weight(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Relu_With_Weight, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.b = self.add_weight(shape=(input_shape[-1],),\n",
    "                                initializer=RandomNormal(),\n",
    "                                trainable=True)\n",
    "        super(Relu_With_Weight, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        return K.tanh(x * self.b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "n_samples = 10000\n",
    "lower_bound = 0\n",
    "upper_bound = 1\n",
    "# lower_bound = -10\n",
    "# upper_bound = 10\n",
    "def custom_function(x):\n",
    "    return np.exp(-2*x) * np.cos(5*np.pi*x) + x\n",
    "\n",
    "X = np.random.uniform(lower_bound, upper_bound, size=(n_samples, 1))\n",
    "#X = np.arange(lower_bound, upper_bound, 0.001)\n",
    "y = custom_function(X)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "def build_model(input_shape, filters):\n",
    "    rank = 3\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = input_layer\n",
    "\n",
    "    h1 = H1Layer()\n",
    "    h2 = H2Layer(h1)\n",
    "    h3 = H3Layer(h1,h2)\n",
    "    h4 = H4Layer(h2,h3)\n",
    "    x = Dense(filters)(x)\n",
    "    x = h2(x)\n",
    "    x = Dense(filters)(x)\n",
    "    x = TensorDecompositionLayer(rank)(x)\n",
    "    x = h3(x)\n",
    "    x = Dense(filters)(x)\n",
    "    x = TensorDecompositionLayer(rank)(x)\n",
    "    x = h4(x)\n",
    "    x = Dense(filters)(x)\n",
    "    x = TensorDecompositionLayer(rank)(x)\n",
    "    \n",
    "    output_layer = Dense(1)(x)\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    return model \n",
    "\n",
    "\n",
    "input_shape = (1,)\n",
    "filters = 64\n",
    "modelN4 = build_model(input_shape, filters)\n",
    "modelN4.summary()\n",
    "optimizer = Adam(learning_rate=0.001) # Reduce learning rate\n",
    "modelN4.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 150\n",
    "\n",
    "history = modelN4.fit(X_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    callbacks=[callback])\n",
    "\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "val_loss = modelN4.evaluate(X_val, y_val, verbose=0)\n",
    "print(f\"Validation loss: {val_loss}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_test_samples = 1000\n",
    "X_test = np.linspace(lower_bound, upper_bound, num=num_test_samples).reshape(-1, 1)\n",
    "\n",
    "y_true = custom_function(X_test)\n",
    "\n",
    "y_pred = modelN4.predict(X_test)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(X_test, y_true, label='True Cosine Values', color='b', linewidth=2)\n",
    "plt.plot(X_test, y_pred, label='Model Predictions', color='r', linestyle='--', linewidth=2)\n",
    "plt.xlabel('Input Value')\n",
    "plt.ylabel('Cosine Value')\n",
    "plt.title('Cosine Function and Model Predictions')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Number of epochs actually trained\n",
    "actual_epochs = len(history.history['loss'])\n",
    "\n",
    "# Plot the loss graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, actual_epochs + 1), train_loss[:actual_epochs], label='Training Loss', color='b', linewidth=2)\n",
    "plt.plot(range(1, actual_epochs + 1), history.history['val_loss'], label='Validation Loss', color='r', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Over Time')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
