{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras import datasets,models,layers\n",
    "# Adding TF Cifar10 Data ..\n",
    "from keras.datasets import cifar10\n",
    "(X_train, Y_train), (X_test, Y_test) = cifar10.load_data()\n",
    "# Normalize the data.\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255.0\n",
    "X_test /= 255.0\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.2,shuffle = True)\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "encoder.fit(Y_train)\n",
    "Y_train = encoder.transform(Y_train).toarray()\n",
    "Y_test = encoder.transform(Y_test).toarray()\n",
    "Y_val =  encoder.transform(Y_val).toarray()\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "aug = ImageDataGenerator(horizontal_flip=True, width_shift_range=0.05,\n",
    "                             height_shift_range=0.05)\n",
    "aug.fit(X_train)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "ResNet-18\n",
    "Reference:\n",
    "[1] K. He et al. Deep Residual Learning for Image Recognition. CVPR, 2016\n",
    "[2] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectifiers:\n",
    "Surpassing human-level performance on imagenet classification. In\n",
    "ICCV, 2015.\n",
    "\"\"\"\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense, Conv2D,  MaxPool2D, Flatten, GlobalAveragePooling2D,  BatchNormalization, Layer, Add\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class ResnetBlock(Model):\n",
    "    \"\"\"\n",
    "    A standard resnet block.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels: int, down_sample=False):\n",
    "        \"\"\"\n",
    "        channels: same as number of convolution kernels\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.__channels = channels\n",
    "        self.__down_sample = down_sample\n",
    "        self.__strides = [2, 1] if down_sample else [1, 1]\n",
    "\n",
    "        KERNEL_SIZE = (3, 3)\n",
    "        # use He initialization, instead of Xavier (a.k.a 'glorot_uniform' in Keras), as suggested in [2]\n",
    "        INIT_SCHEME = \"he_normal\"\n",
    "\n",
    "        self.conv_1 = Conv2D(self.__channels, strides=self.__strides[0],\n",
    "                             kernel_size=KERNEL_SIZE, padding=\"same\", kernel_initializer=INIT_SCHEME)\n",
    "        self.bn_1 = BatchNormalization()\n",
    "        self.conv_2 = Conv2D(self.__channels, strides=self.__strides[1],\n",
    "                             kernel_size=KERNEL_SIZE, padding=\"same\", kernel_initializer=INIT_SCHEME)\n",
    "        self.bn_2 = BatchNormalization()\n",
    "        self.merge = Add()\n",
    "\n",
    "        if self.__down_sample:\n",
    "            # perform down sampling using stride of 2, according to [1].\n",
    "            self.res_conv = Conv2D(\n",
    "                self.__channels, strides=2, kernel_size=(1, 1), kernel_initializer=INIT_SCHEME, padding=\"same\")\n",
    "            self.res_bn = BatchNormalization()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        res = inputs\n",
    "\n",
    "        x = self.conv_1(inputs)\n",
    "        x = self.bn_1(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.conv_2(x)\n",
    "        x = self.bn_2(x)\n",
    "\n",
    "        if self.__down_sample:\n",
    "            res = self.res_conv(res)\n",
    "            res = self.res_bn(res)\n",
    "\n",
    "        # if not perform down sample, then add a shortcut directly\n",
    "        x = self.merge([x, res])\n",
    "        out = tf.nn.relu(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet18(Model):\n",
    "\n",
    "    def __init__(self, num_classes, **kwargs):\n",
    "        \"\"\"\n",
    "            num_classes: number of classes in specific classification task.\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.conv_1 = Conv2D(64, (7, 7), strides=2,\n",
    "                             padding=\"same\", kernel_initializer=\"he_normal\")\n",
    "        self.init_bn = BatchNormalization()\n",
    "        self.pool_2 = MaxPool2D(pool_size=(2, 2), strides=2, padding=\"same\")\n",
    "        self.res_1_1 = ResnetBlock(64)\n",
    "        self.res_1_2 = ResnetBlock(64)\n",
    "        self.res_2_1 = ResnetBlock(128, down_sample=True)\n",
    "        self.res_2_2 = ResnetBlock(128)\n",
    "        self.res_3_1 = ResnetBlock(256, down_sample=True)\n",
    "        self.res_3_2 = ResnetBlock(256)\n",
    "        self.res_4_1 = ResnetBlock(512, down_sample=True)\n",
    "        self.res_4_2 = ResnetBlock(512)\n",
    "        self.avg_pool = GlobalAveragePooling2D()\n",
    "        self.flat = Flatten()\n",
    "        self.fc = Dense(num_classes, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        out = self.conv_1(inputs)\n",
    "        out = self.init_bn(out)\n",
    "        out = tf.nn.relu(out)\n",
    "        out = self.pool_2(out)\n",
    "        for res_block in [self.res_1_1, self.res_1_2, self.res_2_1, self.res_2_2, self.res_3_1, self.res_3_2, self.res_4_1, self.res_4_2]:\n",
    "            out = res_block(out)\n",
    "        out = self.avg_pool(out)\n",
    "        out = self.flat(out)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "model = ResNet18(10)\n",
    "model.build(input_shape = (None,32,32,3))\n",
    "#use categorical_crossentropy since the label is one-hot encoded\n",
    "from keras.optimizers import SGD\n",
    "# opt = SGD(learning_rate=0.1,momentum=0.9,decay = 1e-04) #parameters suggested by He [1]\n",
    "model.compile(optimizer = \"adam\",loss='categorical_crossentropy', metrics=[\"accuracy\"]) \n",
    "model.summary()\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "es = EarlyStopping(patience= 8, restore_best_weights=True, monitor=\"val_acc\")\n",
    "#I did not use cross validation, so the validate performance is not accurate.\n",
    "STEPS = len(X_train) / 256\n",
    "history = model.fit(aug.flow(X_train,Y_train,batch_size = 256), steps_per_epoch=STEPS, batch_size = 256, epochs=30, validation_data=(X_train, Y_train),callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotmodelhistory(history): \n",
    "    fig, axs = plt.subplots(1,2,figsize=(15,5)) \n",
    "    # summarize history for accuracy\n",
    "    axs[0].plot(history.history['accuracy']) \n",
    "    axs[0].plot(history.history['val_accuracy']) \n",
    "    axs[0].set_title('Model Accuracy')\n",
    "    axs[0].set_ylabel('Accuracy') \n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    \n",
    "    axs[0].legend(['train', 'validate'], loc='upper left')\n",
    "    # summarize history for loss\n",
    "    axs[1].plot(history.history['loss']) \n",
    "    axs[1].plot(history.history['val_loss']) \n",
    "    axs[1].set_title('Model Loss')\n",
    "    axs[1].set_ylabel('Loss') \n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].legend(['train', 'validate'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "plotmodelhistory(history)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
