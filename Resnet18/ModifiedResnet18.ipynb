{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n",
    "from tensorflow.keras.layers import Input, Add, Dense, Layer, Activation, concatenate,Conv2D, Flatten, GlobalAveragePooling2D ,BatchNormalization, Dropout\n",
    "from keras.regularizers import l2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "# Clear the session\n",
    "# K.clear_session()\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "class PrintShapeCallback(Callback):\n",
    "    def __init__(self, model_layer_name):\n",
    "        super(PrintShapeCallback, self).__init__()\n",
    "        self.model_layer_name = model_layer_name\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Get the output of the layer with the specified name\n",
    "        layer_output = self.model.get_layer(self.model_layer_name).output\n",
    "        print(f\"After epoch {epoch+1}, shape of x (from layer {self.model_layer_name}): {layer_output.shape}\")\n",
    "\n",
    "print_shape_callback = PrintShapeCallback(model_layer_name='dense_1')  # Assuming 'dense_1' is the name of the Dense layer after h2(x)\n",
    "\n",
    "class StopAtThresholdCallback(Callback):\n",
    "    def __init__(self, threshold):\n",
    "        super(StopAtThresholdCallback, self).__init__()\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        val_loss = logs.get('val_loss')\n",
    "        if val_loss is not None and val_loss < self.threshold:\n",
    "            print(f\"\\nStopping training as validation loss {val_loss} is below the threshold {self.threshold}\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "# callback = StopAtThresholdCallback(threshold=1e-03)\n",
    "callback = StopAtThresholdCallback(threshold=9.8023e-06)\n",
    "\n",
    "class H1Layer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(H1Layer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.b = self.add_weight(shape=(input_shape[-1],),\n",
    "                                initializer=RandomNormal(mean=0.0,stddev=0.03),\n",
    "                                trainable=True)\n",
    "        super(H1Layer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        return (self.b * (2 * x))/ (2**(1/2) * np.pi**(1/4))\n",
    "        #return (2 * x) \n",
    "\n",
    "\n",
    "class H2Layer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(H2Layer, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, x, h1):\n",
    "        return ((2*x*(h1))-2)/(2*(np.pi**(1/4))*np.sqrt(math.factorial(2)))\n",
    "    \n",
    "class H3Layer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(H3Layer, self).__init__(**kwargs)\n",
    "        \n",
    "    def call(self, x, h1, h2):\n",
    "        return ((2 * x * (h2))-(4 * h1)) / (2**(3/2)*(np.pi**(1/4))*np.sqrt(math.factorial(3)))\n",
    "\n",
    "class H4Layer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(H4Layer, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, x, h2, h3):\n",
    "        return ((2*x*(h3))-(6*h2)) / (2**2 *(np.pi**(1/4))*np.sqrt(math.factorial(4)))\n",
    "\n",
    "class H5Layer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(H5Layer,self).__init__(**kwargs)\n",
    "\n",
    "    def call(self,x, h3, h4):\n",
    "        return ((2*x*(h4))-(8*h3)) / (2**(5/2) * (np.pi**(1/4))*np.sqrt(math.factorial(5)))\n",
    "    \n",
    "class H6Layer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(H6Layer,self).__init__(**kwargs)\n",
    "        \n",
    "    def call(self,x, h4, h5):\n",
    "        return (2*x*(h5))-(10*h4)\n",
    "\n",
    "class TensorDecompositionLayer(Layer):\n",
    "    def __init__(self, rank, **kwargs):\n",
    "        self.rank = rank\n",
    "        super(TensorDecompositionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.factors_a = self.add_weight(shape=(input_shape[-1], self.rank),\n",
    "                                         initializer=RandomNormal(mean=0.0,stddev=0.05),\n",
    "                                         trainable=True)\n",
    "        self.factors_b = self.add_weight(shape=(self.rank, input_shape[-1]),\n",
    "                                        initializer=RandomNormal(mean=0.0,stddev=0.05),\n",
    "                                        trainable=True)\n",
    "        super(TensorDecompositionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        return tf.matmul(tf.matmul(x, self.factors_a), self.factors_b)\n",
    "\n",
    " \n",
    "class Relu_With_Weight(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Relu_With_Weight, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.b = self.add_weight(shape=(input_shape[-1],),\n",
    "                                initializer=RandomNormal(),\n",
    "                                trainable=True)\n",
    "        super(Relu_With_Weight, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        return K.tanh(x * self.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras import datasets,models,layers\n",
    "# Adding TF Cifar10 Data ..\n",
    "from keras.datasets import cifar10\n",
    "(X_train, Y_train), (X_test, Y_test) = cifar10.load_data()\n",
    "# Normalize the data.\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255.0\n",
    "X_test /= 255.0\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.2,shuffle = True)\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "encoder.fit(Y_train)\n",
    "Y_train = encoder.transform(Y_train).toarray()\n",
    "Y_test = encoder.transform(Y_test).toarray()\n",
    "Y_val =  encoder.transform(Y_val).toarray()\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "aug = ImageDataGenerator(horizontal_flip=True, width_shift_range=0.05,\n",
    "                             height_shift_range=0.05)\n",
    "aug.fit(X_train)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "ResNet-18\n",
    "Reference:\n",
    "[1] K. He et al. Deep Residual Learning for Image Recognition. CVPR, 2016\n",
    "[2] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectifiers:\n",
    "Surpassing human-level performance on imagenet classification. In\n",
    "ICCV, 2015.\n",
    "\"\"\"\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense, Conv2D,  MaxPool2D, Flatten, GlobalAveragePooling2D,  BatchNormalization, Layer, Add\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class ResNetHermiteBlock(Model):\n",
    "    \"\"\"\n",
    "    A HERMITE resnet block.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels: int, down_sample=False):\n",
    "        \"\"\"\n",
    "        channels: same as number of convolution kernels\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.__channels = channels\n",
    "        self.__down_sample = down_sample\n",
    "        self.__strides = [2, 1] if down_sample else [1, 1]\n",
    "\n",
    "        KERNEL_SIZE = (3, 3)\n",
    "        # use He initialization, instead of Xavier (a.k.a 'glorot_uniform' in Keras), as suggested in [2]\n",
    "        INIT_SCHEME = \"he_normal\"\n",
    "\n",
    "        self.conv_1 = Conv2D(self.__channels, strides=self.__strides[0],\n",
    "                             kernel_size=KERNEL_SIZE, padding=\"same\", kernel_initializer=INIT_SCHEME)\n",
    "        self.bn_1 = BatchNormalization()\n",
    "        self.conv_2 = Conv2D(self.__channels, strides=self.__strides[1],\n",
    "                             kernel_size=KERNEL_SIZE, padding=\"same\", kernel_initializer=INIT_SCHEME)\n",
    "        self.bn_2 = BatchNormalization()\n",
    "        self.merge = Add()\n",
    "\n",
    "        self.tensorDecomp = TensorDecompositionLayer(3)\n",
    "        # Initialize Hermite Polynomial layers\n",
    "        self.h1 = H1Layer()\n",
    "        self.h2 = H2Layer()\n",
    "\n",
    "        if self.__down_sample:\n",
    "            # perform down sampling using stride of 2, according to [1].\n",
    "            self.res_conv = Conv2D(\n",
    "                self.__channels, strides=2, kernel_size=(1, 1), kernel_initializer=INIT_SCHEME, padding=\"same\")\n",
    "            self.res_bn = BatchNormalization()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        res = inputs\n",
    "\n",
    "        x = self.conv_1(inputs)\n",
    "        # x = self.bn_1(x)\n",
    "        x = x_h1 = self.h1(x)  # Store H1 output if needed for H2\n",
    "        # x = tf.nn.relu(x)\n",
    "        x = self.conv_2(x)\n",
    "        # x = self.bn_2(x)\n",
    "        x = x_h2 = self.h2(x, x_h1)  # Pass x_h1 as the additional argument to H2Layer\n",
    "        x = self.tensorDecomp(x)\n",
    "\n",
    "\n",
    "        if self.__down_sample:\n",
    "            res = self.res_conv(res)\n",
    "            res = self.res_bn(res)\n",
    "\n",
    "        # if not perform down sample, then add a shortcut directly\n",
    "        out = self.merge([x, res])\n",
    "        # out = tf.nn.relu(x)\n",
    "        # x_h1 = self.h1(x)  # Store H1 output if needed for H2\n",
    "        # out = x_h2 = self.h2(x, x_h1)  # Pass x_h1 as the additional argument to H2Layer\n",
    "        # out = self.tensorDecomp(x)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet18(Model):\n",
    "\n",
    "    def __init__(self, num_classes, **kwargs):\n",
    "        \"\"\"\n",
    "            num_classes: number of classes in specific classification task.\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.conv_1 = Conv2D(64, (7, 7), strides=2,\n",
    "                             padding=\"same\", kernel_initializer=\"he_normal\")\n",
    "        self.init_bn = BatchNormalization()\n",
    "        self.pool_2 = MaxPool2D(pool_size=(2, 2), strides=2, padding=\"same\")\n",
    "        self.res_1_1 = ResNetHermiteBlock(64)\n",
    "        self.res_1_2 = ResNetHermiteBlock(64)\n",
    "        self.res_2_1 = ResNetHermiteBlock(128, down_sample=True)\n",
    "        self.res_2_2 = ResNetHermiteBlock(128)\n",
    "        self.res_3_1 = ResNetHermiteBlock(256, down_sample=True)\n",
    "        self.res_3_2 = ResNetHermiteBlock(256)\n",
    "        self.res_4_1 = ResNetHermiteBlock(512, down_sample=True)\n",
    "        self.res_4_2 = ResNetHermiteBlock(512)\n",
    "        self.avg_pool = GlobalAveragePooling2D()\n",
    "        self.flat = Flatten()\n",
    "        self.fc = Dense(num_classes, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        out = self.conv_1(inputs)\n",
    "        out = self.init_bn(out)\n",
    "        # out = tf.nn.relu(out)\n",
    "        out = self.pool_2(out)\n",
    "\n",
    "        for res_block in [self.res_1_1, self.res_1_2, self.res_2_1, self.res_2_2, self.res_3_1, self.res_3_2, self.res_4_1, self.res_4_2]:\n",
    "        # for res_block in [self.res_1_1, self.res_1_2, self.res_2_1, self.res_2_2, self.res_3_1, self.res_3_2 ,self.res_4_1]:\n",
    "            out = res_block(out)\n",
    "        out = self.avg_pool(out)\n",
    "        out = self.flat(out)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = ResNet18(10)\n",
    "model.build(input_shape = (None,32,32,3))\n",
    "#use categorical_crossentropy since the label is one-hot encoded\n",
    "from keras.optimizers import SGD\n",
    "# opt = SGD(learning_rate=0.1,momentum=0.9,decay = 1e-04) #parameters suggested by He [1]\n",
    "model.compile(optimizer = \"adam\",loss='categorical_crossentropy', metrics=[\"accuracy\"]) #LR = 0.001\n",
    "model.summary()\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "es = EarlyStopping(patience= 8, restore_best_weights=True, monitor=\"val_accuracy\")\n",
    "#I did not use cross validation, so the validate performance is not accurate.\n",
    "STEPS = len(X_train) / 64\n",
    "# history = model.fit(aug.flow(X_train,Y_train,batch_size = 256), steps_per_epoch=STEPS, batch_size = 256, epochs=50, validation_data=(X_train, Y_train),callbacks=[es])\n",
    "history = model.fit(aug.flow(X_train,Y_train,batch_size = 64), steps_per_epoch=STEPS, batch_size = 64, epochs=50, validation_data=(X_train, Y_train))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
